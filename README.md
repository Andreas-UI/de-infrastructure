# Data Engineering Infrastructure

This repository provides a comprehensive local development environment for simple data engineering projects. It includes pre-configured services commonly used in production data pipelines:

* Apache Airflow
* Apache Kafka
* Apache Spark
* Redis
* PostgreSQL

Each service is containerized with Docker for easy setup and teardown for prototyping and integration testing. You may or may not use all of the services included here ‚Äî they are technologies I have used in past projects. Feel free to remove or add services based on your own project requirements.

## üöÄ Getting Started

### Prerequisites

* [Docker Desktop](https://www.docker.com/products/docker-desktop) must be installed and running.
* [Make](https://www.gnu.org/software/make/) utility should be available on your system.

### Commands

```bash
make compose-up     # Start all services
make compose-down   # Stop all services
make clean          # Stop and remove all services, volumes, and generated data (use with caution)
```

## üîó Integration with Your Data Engineering Projects

This infrastructure is designed to be reused across all your data engineering (DE) projects. Instead of spinning up separate environments for each project, this setup serves as a central, one-to-many infrastructure, making development more streamlined and efficient.

### ‚úÖ Recommended Integration Steps

1. Clone this repository once to your local machine:

    ```bash
    https://github.com/Andreas-UI/de-infrastructure
    ```

    Place it somewhere you can remember easily, as you would need a path to this infrastructure, to override the docker-compose for each project. A suggested structure:

    ```css
    ~/DE-Projects/
    ‚îú‚îÄ‚îÄ de-infra/          # This repo
    ‚îú‚îÄ‚îÄ project-a/
    ‚îú‚îÄ‚îÄ project-b/
    ‚îî‚îÄ‚îÄ project-c/
    ```

2. Add Environment File
    Create a file `.env` and paste the value to this file

    ```bash
    AIRFLOW_UID=5000   # Example
    ```

3. Start the infrastructure:

    * Make sure Docker Desktop is running.
    * Navigate to the de-infra/ directory.
    * Run:

    ```bash
    make compose-up
    ```

4. Use in your DE projects:

    Your other DE projects can connect to these services (e.g., Airflow, Spark, Kafka) by referencing localhost (if you are running locally) and the corresponding exposed ports.

> This approach keeps your environments clean, consistent, and efficient, avoiding duplication and reducing configuration overhead across projects.

## üì¶ Services Overview

### 1. Apache Airflow (v3.0.2)

Orchestrate and schedule workflows
References:

* [Main Site](https://airflow.apache.org/)
* [Documentation](https://airflow.apache.org/docs/)
* [Running Airflow in Docker](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html)

### 2. Apache Kafka (Latest)

Real-time message broker for building streaming data pipelines.
References:

* [Main](https://kafka.apache.org/)
* [Quickstart](https://kafka.apache.org/quickstart)
* [Docker Image](https://hub.docker.com/r/apache/kafka)
* [Documentation](https://kafka.apache.org/documentation/)
* [Documentation (Beginner)](https://learn.conduktor.io/kafka/)

### 3. Apache Spark (v4.0.0)

Distributed processing engine for big data, streaming and batch jobs.
References:

* [Main](https://spark.apache.org/)
* [Documentation](https://spark.apache.org/docs/4.0.0/)
* [Pyspark](https://spark.apache.org/docs/4.0.0/api/python/index.html)

### 4. Redis (v7.2-bookworm)

In-memory key-value store often used for caching or message queues.
References:

* [Main](https://redis.io/)

### 5. Postgresql

Relational database for storing structured data. Also used by Airflow for metadata.

* [Main](<https://www.postgresql.org/>)

## üìÅ Directory Structure

> ‚ö†Ô∏è Do not manually modify the following directories unless you understand their function.

| Directory | Description |
|---|---|
| `config/` | Airflow configuration. |
| `dags/` | DAG definitions for Apache Airflow. |
| `kafka-data/` | Persistent data for Kafka topics and logs. |
| `logs/` | Runtime logs generated by Airflow. |
| `plugins/` | Custom Airflow plugins (operators, hooks, etc.). |
| `spark_jobs/` | PySpark or Spark jobs submitted to the Spark engine. |
| `.env` | Environment variables used to configure services. (Ignored by git) |

## üì¢ Notes

This setup is intended for local development only. Do not expose credentials or production data in this repository. For best practices in production deployment, refer to each service's official documentation.
